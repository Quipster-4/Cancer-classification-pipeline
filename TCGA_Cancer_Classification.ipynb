{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "data_folder = 'tcga_data'\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    print(f\"üìÇ '{data_folder}' not found. Downloading dataset from Google Drive...\")\n",
    "    \n",
    "    \n",
    "    file_id = '1nfeA7qWkOYu9G9N8nwQ85g-vX7yt5ENx' \n",
    "\n",
    "    \n",
    "    url = f'https://drive.google.com/uc?id=1nfeA7qWkOYu9G9N8nwQ85g-vX7yt5ENx'\n",
    "    output_zip = 'dataset.zip'\n",
    "    \n",
    "    # Installing gdown for reliable Drive downloads\n",
    "    !pip install -U -q gdown\n",
    "    import gdown\n",
    "    \n",
    "    # Downloading \n",
    "    gdown.download(url, output_zip, quiet=False)\n",
    "    \n",
    "    # Unzipping \n",
    "    print(\"Unzipping data...\")\n",
    "    !unzip -q dataset.zip\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(output_zip)\n",
    "    print(\"‚úÖ Setup complete! Dataset is ready.\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already exists. Proceeding...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23670,
     "status": "ok",
     "timestamp": 1764264994697,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "ReLNizFZRE2v",
    "outputId": "05f727a2-f540-4ca4-de42-569ca76b18e0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === CANCER GENOMIC CLASSIFICATION WITH SPARK ===\n",
    "# Complete starter code for TCGA data classification\n",
    "\n",
    "# Installing required packages\n",
    "!pip install pyspark\n",
    "!pip install plotly\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initializing Spark Session\n",
    "print(\"üöÄ Initializing Spark Session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CancerGenomicClassification\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20501,
     "status": "ok",
     "timestamp": 1764265025429,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "yKJMfAMTSDTD",
    "outputId": "1f0f72a5-4a1f-438e-f51b-2d00465f6d71"
   },
   "outputs": [],
   "source": [
    "# Creating synthetic genomic data that mimics TCGA format\n",
    "print(\"üß¨ Creating synthetic TCGA-like genomic data...\")\n",
    "\n",
    "def create_synthetic_tcga_data(num_samples=1000, num_genes=500):\n",
    "    \"\"\"Create synthetic genomic data resembling TCGA dataset\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Creating gene names (e.g., BRCA1, TP53, etc.)\n",
    "    gene_names = [f'GENE_{i:04d}' for i in range(num_genes)]\n",
    "\n",
    "    # Creating sample IDs\n",
    "    sample_ids = [f'TCGA-{i:04d}' for i in range(num_samples)]\n",
    "\n",
    "    # Creating synthetic expression data\n",
    "    # Different cancer types have different expression patterns\n",
    "    data = []\n",
    "    for i, sample_id in enumerate(sample_ids):\n",
    "        # Assigning cancer type based on sample index\n",
    "        cancer_type = i % 5  # 5 different cancer types\n",
    "\n",
    "        # Creating expression profile with cancer-specific patterns\n",
    "        base_expression = np.random.normal(0, 1, num_genes)\n",
    "\n",
    "        # Adding cancer-specific signature\n",
    "        if cancer_type == 0:  # Breast cancer-like\n",
    "            base_expression[0:50] += np.random.normal(2, 0.5, 50)\n",
    "        elif cancer_type == 1:  # Lung cancer-like\n",
    "            base_expression[50:100] += np.random.normal(1.5, 0.5, 50)\n",
    "        elif cancer_type == 2:  # Prostate cancer-like\n",
    "            base_expression[100:150] += np.random.normal(1.8, 0.5, 50)\n",
    "        elif cancer_type == 3:  # Colon cancer-like\n",
    "            base_expression[150:200] += np.random.normal(2.2, 0.5, 50)\n",
    "        else:  # Brain cancer-like\n",
    "            base_expression[200:250] += np.random.normal(1.2, 0.5, 50)\n",
    "\n",
    "        # Adding some noise\n",
    "        noise = np.random.normal(0, 0.3, num_genes)\n",
    "        expression = base_expression + noise\n",
    "\n",
    "        # Creating row\n",
    "        row = [sample_id, cancer_type] + expression.tolist()\n",
    "        data.append(row)\n",
    "\n",
    "    # Creating columns\n",
    "    columns = ['sample_id', 'cancer_type'] + gene_names\n",
    "\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Generating synthetic data\n",
    "synthetic_data = create_synthetic_tcga_data(1000, 500)\n",
    "print(f\"‚úÖ Created synthetic dataset with {len(synthetic_data)} samples and {len(synthetic_data.columns)-2} genes\")\n",
    "\n",
    "# Converting to Spark DataFrame\n",
    "genomic_df = spark.createDataFrame(synthetic_data)\n",
    "print(\"üìä Data overview:\")\n",
    "genomic_df.show(5)\n",
    "genomic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3804,
     "status": "ok",
     "timestamp": 1764265029236,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "sEWdsg_1SSL9",
    "outputId": "0b34833b-1f7b-45a6-eafb-9c37ae44a2ff"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing and feature engineering\n",
    "print(\"üîß Setting up data preprocessing pipeline...\")\n",
    "\n",
    "# Converting cancer_type to string labels for better interpretation\n",
    "cancer_labels = ['Breast_Cancer', 'Lung_Cancer', 'Prostate_Cancer', 'Colon_Cancer', 'Brain_Cancer']\n",
    "label_mapping = {i: label for i, label in enumerate(cancer_labels)}\n",
    "\n",
    "def map_cancer_type(cancer_idx):\n",
    "    return cancer_labels[cancer_idx]\n",
    "\n",
    "# Registering UDF for Spark\n",
    "from pyspark.sql.types import StringType\n",
    "map_cancer_udf = udf(map_cancer_type, StringType())\n",
    "\n",
    "# Applying cancer type labels\n",
    "labeled_df = genomic_df.withColumn(\"cancer_label\", map_cancer_udf(col(\"cancer_type\")))\n",
    "print(\"üéØ Cancer type distribution:\")\n",
    "labeled_df.groupBy(\"cancer_label\").count().show()\n",
    "\n",
    "# Preparing features (all gene columns)\n",
    "feature_columns = [col for col in genomic_df.columns if col.startswith('GENE_')]\n",
    "print(f\"üß¨ Using {len(feature_columns)} genomic features\")\n",
    "\n",
    "# Creating preprocessing pipeline stages\n",
    "print(\"‚öôÔ∏è Creating ML pipeline...\")\n",
    "\n",
    "# Converting string label to numeric index\n",
    "label_indexer = StringIndexer(inputCol=\"cancer_label\", outputCol=\"label\")\n",
    "\n",
    "# Assembling features\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"raw_features\")\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaledFeatures\",\n",
    "                       withStd=True, withMean=True)\n",
    "\n",
    "# Applying PCA for dimensionality reduction\n",
    "pca = PCA(k=50, inputCol=\"scaledFeatures\", outputCol=\"features\")\n",
    "\n",
    "print(\"‚úÖ Pipeline stages defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69368,
     "status": "ok",
     "timestamp": 1764265098652,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "0mrDCXFuSriI",
    "outputId": "b4661c4e-57bb-4432-f82e-0ea8c0725771"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "train_data, test_data = labeled_df.randomSplit([0.7, 0.3], seed=42)\n",
    "print(f\"üìö Training samples: {train_data.count()}\")\n",
    "print(f\"üß™ Test samples: {test_data.count()}\")\n",
    "\n",
    "# Defining models\n",
    "print(\"ü§ñ Initializing machine learning models...\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\",\n",
    "                           numTrees=100, maxDepth=10, seed=42)\n",
    "\n",
    "# Creating pipelines\n",
    "pipeline_rf = Pipeline(stages=[label_indexer, assembler, scaler, pca, rf])\n",
    "\n",
    "# Training Random Forest model\n",
    "print(\"üå≤ Training Random Forest...\")\n",
    "model_rf = pipeline_rf.fit(train_data)\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17099,
     "status": "ok",
     "timestamp": 1764265115738,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "19V90BLuUMb2",
    "outputId": "2879d656-97a8-4d58-f6ca-1c47c497f863"
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "print(\"üìä Evaluating models...\")\n",
    "\n",
    "predictions_rf = model_rf.transform(test_data)\n",
    "\n",
    "# Evaluating models\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "metrics = ['accuracy', 'weightedPrecision', 'weightedRecall', 'f1']\n",
    "\n",
    "results = {}\n",
    "for metric in metrics:\n",
    "    rf_score = evaluator.evaluate(predictions_rf, {evaluator.metricName: metric})\n",
    "    results[metric] = {'Random Forest': rf_score}\n",
    "\n",
    "# Displaying results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ MODEL PERFORMANCE RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìà BEST PERFORMING MODEL BY METRIC\")\n",
    "print(\"=\"*50)\n",
    "for metric in metrics:\n",
    "    # Since there's only one model now, we can directly get its name and score\n",
    "    model_name = list(results[metric].keys())[0]\n",
    "    model_score = results[metric][model_name]\n",
    "    print(f\"{metric:>20}: {model_name} ({model_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1764265115780,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "vYWz0s0CVa2v",
    "outputId": "66569d3b-9309-48ff-a0ea-01c8b9cad597"
   },
   "outputs": [],
   "source": [
    "# Extracting feature importance from Random Forest\n",
    "print(\"\\nüå≥ Analyzing feature importance...\")\n",
    "\n",
    "rf_model = model_rf.stages[-1]  # Get the RandomForest model\n",
    "feature_importances = rf_model.featureImportances\n",
    "\n",
    "# Getting top 20 most important features\n",
    "importance_list = [(i, float(importance)) for i, importance in enumerate(feature_importances)]\n",
    "importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nüîù Top 20 Most Important Genomic Features:\")\n",
    "print(\"Rank | Feature Index | Importance\")\n",
    "print(\"-\" * 40)\n",
    "for rank, (idx, importance) in enumerate(importance_list[:20], 1):\n",
    "    print(f\"{rank:4} | {idx:13} | {importance:.6f}\")\n",
    "\n",
    "# PCA Analysis\n",
    "pca_model = model_rf.stages[3]  # Get the PCA model\n",
    "print(f\"\\nüìâ PCA Explained Variance: {pca_model.explainedVariance.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6421,
     "status": "ok",
     "timestamp": 1764265122207,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "rzhGC4MdVw7_",
    "outputId": "f36ad4ba-7827-4a8a-f183-25cbfda9cd95"
   },
   "outputs": [],
   "source": [
    "# Creating visualizations\n",
    "print(\"‚ÑπÔ∏è Creating visualizations...\")\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    # Converting predictions to Pandas for visualization\n",
    "    pdf_rf = predictions_rf.select(\"label\", \"prediction\", \"probability\").toPandas()\n",
    "\n",
    "    # Confusion Matrix Data\n",
    "    confusion_data = pdf_rf.groupby(['label', 'prediction']).size().reset_index(name='count')\n",
    "\n",
    "    # Creating confusion matrix heatmap\n",
    "    fig = px.density_heatmap(confusion_data, x='prediction', y='label', z='count',\n",
    "                            title='Confusion Matrix - Random Forest',\n",
    "                            color_continuous_scale='Blues')\n",
    "    fig.show()\n",
    "\n",
    "    # Model comparison bar chart\n",
    "    # Only Random Forest is available now\n",
    "    models = ['Random Forest']\n",
    "    accuracy_scores = [results['accuracy']['Random Forest']]\n",
    "\n",
    "    fig2 = px.bar(x=models, y=accuracy_scores,\n",
    "                 title='Model Accuracy Comparison',\n",
    "                 labels={'x': 'Model', 'y': 'Accuracy'},\n",
    "                 color=accuracy_scores, color_continuous_scale='Viridis')\n",
    "    fig2.show()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Plotly not available for visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17330,
     "status": "ok",
     "timestamp": 1764265139543,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "ocWh7s11WOq2",
    "outputId": "a125bd0a-9a81-4348-8595-6beedd380bea"
   },
   "outputs": [],
   "source": [
    "# Saving the model and results\n",
    "print(\"\\nüíæ Saving model and results...\")\n",
    "\n",
    "# Saving the trained model\n",
    "model_rf.write().overwrite().save(\"random_forest_cancer_model\")\n",
    "\n",
    "# Saving predictions\n",
    "predictions_rf.select(\"sample_id\", \"cancer_label\", \"prediction\") \\\n",
    "             .write.mode(\"overwrite\").csv(\"cancer_predictions\", header=True)\n",
    "\n",
    "print(\"‚úÖ Model and predictions saved successfully!\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4411,
     "status": "ok",
     "timestamp": 1764265144002,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "ycXqX8nbz1a2",
    "outputId": "09d96675-24af-495d-bf0b-5590a5b7c408"
   },
   "outputs": [],
   "source": [
    "# Downloading TCGA data directly to Colab (much faster than uploading)\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Creating the directory if it doesn't exist\n",
    "os.makedirs('/content/tcga_data/', exist_ok=True)\n",
    "\n",
    "# Downloading a TCGA dataset directly\n",
    "tcga_urls = {\n",
    "    \"rna_seq\": \"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/EB%2B%2BAdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena.gz\",\n",
    "    \"clinical\": \"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/Survival_SupplementalTable_S1_20171025_xena_sp\"\n",
    "}\n",
    "\n",
    "for name, url in tcga_urls.items():\n",
    "    print(f\"Downloading {name}...\")\n",
    "    filename = f\"/content/tcga_data/{name}.tsv\"\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"Downloaded: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1764265144807,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "8MvolW6s_7wf",
    "outputId": "28882d0e-e393-456c-e9e0-3a265cc82842"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "import gzip\n",
    "import io\n",
    "\n",
    "# Loading the RNA-Seq data\n",
    "print(\"Loading RNA-Seq data...\")\n",
    "rna_seq_path = \"/content/tcga_data/rna_seq.tsv\"\n",
    "\n",
    "# First, let's check the file content to determine if it's gzipped\n",
    "try:\n",
    "    # Reading a small chunk to check for gzip magic number\n",
    "    with open(rna_seq_path, 'rb') as f:\n",
    "        first_bytes = f.read(2)\n",
    "\n",
    "    if first_bytes == b'\\x1f\\x8b': # Gzip magic number\n",
    "        print(\"Detected gzipped file. Opening with gzip...\")\n",
    "        with gzip.open(rna_seq_path, 'rt') as f:\n",
    "            first_lines = [next(f) for _ in range(3)]\n",
    "    else:\n",
    "        print(\"Detected plain text file. Opening directly...\")\n",
    "        with open(rna_seq_path, 'r') as f:\n",
    "            first_lines = [next(f) for _ in range(3)]\n",
    "\n",
    "    print(\"File structure preview:\")\n",
    "    for i, line in enumerate(first_lines):\n",
    "        print(f\"Line {i}: {line[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {e}\")\n",
    "    # Trying direct pandas read if the above fails\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1764265144938,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "cdkBRxztE9fF",
    "outputId": "c4c0af38-10fa-4841-d92f-e7d8e411fcba"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# Checking if files are actually gzipped\n",
    "print(\"Checking file types...\")\n",
    "for file_path in ['/content/tcga_data/rna_seq.tsv', '/content/tcga_data/clinical.tsv']:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            magic_number = f.read(2)\n",
    "            print(f\"{file_path}: First 2 bytes = {magic_number.hex()}\")\n",
    "            if magic_number == b'\\x1f\\x8b':  # GZIP magic number\n",
    "                print(\"  ‚Üí This is a GZIP file!\")\n",
    "            else:\n",
    "                print(\"  ‚Üí Not a GZIP file\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18337,
     "status": "ok",
     "timestamp": 1764265163282,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "AKE60QcCFF26",
    "outputId": "534a3bda-9e98-44a3-b61c-20a850d5779f"
   },
   "outputs": [],
   "source": [
    "def extract_gzip_file(gzip_path, output_path):\n",
    "    \"\"\"Extract a gzipped file\"\"\"\n",
    "    print(f\"Extracting {gzip_path} to {output_path}...\")\n",
    "    try:\n",
    "        with gzip.open(gzip_path, 'rb') as f_in:\n",
    "            with open(output_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"‚úì Successfully extracted to {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Extraction failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Extracting both files\n",
    "extracted_files = {}\n",
    "for file_type in ['rna_seq', 'clinical']:\n",
    "    gzip_path = f'/content/tcga_data/{file_type}.tsv'\n",
    "    extracted_path = f'/content/tcga_data/{file_type}_extracted.tsv'\n",
    "\n",
    "    if extract_gzip_file(gzip_path, extracted_path):\n",
    "        extracted_files[file_type] = extracted_path\n",
    "\n",
    "print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5340,
     "status": "ok",
     "timestamp": 1764265168629,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "LgbGsRS7KEVA",
    "outputId": "dd57d0df-742b-4fdd-86aa-e8f294214425"
   },
   "outputs": [],
   "source": [
    "def load_tcga_properly(rna_path, sample_fraction=0.3, max_genes=1000):\n",
    "    \"\"\"\n",
    "    Load TCGA data with the correct orientation\n",
    "    \"\"\"\n",
    "    print(\"Loading TCGA data with proper orientation...\")\n",
    "\n",
    "    # Reading the header to get sample IDs\n",
    "    with open(rna_path, 'r') as f:\n",
    "        header = f.readline().strip().split('\\t')\n",
    "        sample_ids = header[1:]  # First column is 'sample', rest are sample IDs\n",
    "        print(f\"Total samples: {len(sample_ids)}\")\n",
    "        print(f\"Sample IDs preview: {sample_ids[:5]}\")\n",
    "\n",
    "    # Calculating how many samples to load\n",
    "    n_samples_to_load = __builtins__.min(int(len(sample_ids) * sample_fraction), 2000)\n",
    "    samples_to_keep = sample_ids[:n_samples_to_load]\n",
    "\n",
    "    # We need to keep the 'sample' column + the selected samples\n",
    "    columns_to_keep = ['sample'] + samples_to_keep\n",
    "    column_indices = [0] + list(range(1, n_samples_to_load + 1))\n",
    "\n",
    "    print(f\"Loading {n_samples_to_load} samples and up to {max_genes} genes\")\n",
    "\n",
    "    # Loading the data - genes as rows, samples as columns\n",
    "    df = pd.read_csv(\n",
    "        rna_path,\n",
    "        sep='\\t',\n",
    "        usecols=column_indices,\n",
    "        nrows=max_genes\n",
    "    )\n",
    "\n",
    "    print(f\"Loaded data shape: {df.shape}\")\n",
    "    print(f\"First few gene IDs: {df['sample'].head(5).tolist()}\")\n",
    "\n",
    "    # Setting gene IDs as index and transpose to get samples as rows\n",
    "    df = df.set_index('sample')\n",
    "    df_transposed = df.T  # Now samples are rows, genes are columns\n",
    "\n",
    "    print(f\"After transpose: {df_transposed.shape}\")\n",
    "    print(f\"Samples: {df_transposed.shape[0]}, Genes: {df_transposed.shape[1]}\")\n",
    "\n",
    "    # Checking for missing values and data types\n",
    "    print(f\"Missing values: {df_transposed.isnull().sum().sum()}\")\n",
    "    print(f\"Data types: {df_transposed.dtypes.value_counts()}\")\n",
    "\n",
    "    return df_transposed\n",
    "\n",
    "# Loading the data properly\n",
    "print(\"=== LOADING DATA WITH CORRECT ORIENTATION ===\")\n",
    "rna_data_correct = load_tcga_properly(\n",
    "    extracted_files['rna_seq'],\n",
    "    sample_fraction=0.3,  # 30% of samples\n",
    "    max_genes=1500        # First 1500 genes\n",
    ")\n",
    "\n",
    "print(\"\\nFirst few rows of corrected data:\")\n",
    "print(rna_data_correct.head(3))\n",
    "print(f\"Sample of values: {rna_data_correct.iloc[0, 0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1764265168690,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "dfphtOiMKsHV",
    "outputId": "324d8fd8-6167-4fef-ccf0-453f7e283b99"
   },
   "outputs": [],
   "source": [
    "# Handling NA values and convert to numeric\n",
    "print(\"=== CLEANING DATA ===\")\n",
    "\n",
    "# Replacing 'NA' strings with actual NaN\n",
    "rna_data_clean = rna_data_correct.replace('NA', np.nan)\n",
    "\n",
    "# Converting all columns to numeric\n",
    "for col in rna_data_clean.columns:\n",
    "    rna_data_clean[col] = pd.to_numeric(rna_data_clean[col], errors='coerce')\n",
    "\n",
    "print(f\"Missing values after cleaning: {rna_data_clean.isnull().sum().sum()}\")\n",
    "print(f\"Data types after conversion: {rna_data_clean.dtypes.value_counts()}\")\n",
    "\n",
    "# Removing rows (samples) with too many missing values\n",
    "initial_samples = rna_data_clean.shape[0]\n",
    "rna_data_clean = rna_data_clean.dropna(thresh=0.8 * rna_data_clean.shape[1])  # Keep samples with at least 80% data\n",
    "final_samples = rna_data_clean.shape[0]\n",
    "\n",
    "print(f\"Removed {initial_samples - final_samples} samples with too many missing values\")\n",
    "print(f\"Final clean data shape: {rna_data_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1764265168718,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "IrulTtocMApJ",
    "outputId": "e7aa56e1-b997-4ec5-eda2-f37c6e552ffd"
   },
   "outputs": [],
   "source": [
    "# Examining what project codes we actually have\n",
    "print(\"=== ANALYZING ACTUAL PROJECT CODES ===\")\n",
    "\n",
    "# Getting all unique project codes from sample IDs\n",
    "sample_ids = rna_data_clean.index.tolist()\n",
    "project_codes = []\n",
    "\n",
    "for sample_id in sample_ids:\n",
    "    parts = sample_id.split('-')\n",
    "    if len(parts) >= 2:\n",
    "        project_codes.append(parts[1])\n",
    "\n",
    "unique_projects = set(project_codes)\n",
    "print(f\"Unique project codes found: {sorted(unique_projects)}\")\n",
    "print(f\"Number of unique projects: {len(unique_projects)}\")\n",
    "\n",
    "# Showing distribution\n",
    "project_counts = {}\n",
    "for project in unique_projects:\n",
    "    project_counts[project] = project_codes.count(project)\n",
    "\n",
    "print(\"\\nProject code distribution:\")\n",
    "for project, count in sorted(project_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {project}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1764265168749,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "W_nuyubfMp7t",
    "outputId": "1fc415ab-ff4b-41d3-f4db-a39eef28b26a"
   },
   "outputs": [],
   "source": [
    "# Creating realistic cancer type labels\n",
    "print(\"=== CREATING MEANINGFUL CANCER TYPE LABELS ===\")\n",
    "\n",
    "def create_realistic_cancer_labels(sample_ids, n_cancer_types=5):\n",
    "    \"\"\"\n",
    "    Create realistic cancer type labels based on TCGA distribution patterns\n",
    "    \"\"\"\n",
    "    # Most common cancer types in TCGA (by approximate frequency)\n",
    "    common_cancers = [\n",
    "        'Breast Invasive Carcinoma (BRCA)',\n",
    "        'Lung Adenocarcinoma (LUAD)',\n",
    "        'Prostate Adenocarcinoma (PRAD)',\n",
    "        'Colon Adenocarcinoma (COAD)',\n",
    "        'Kidney Renal Clear Cell Carcinoma (KIRC)',\n",
    "        'Brain Lower Grade Glioma (LGG)',\n",
    "        'Head and Neck Squamous Cell Carcinoma (HNSC)',\n",
    "        'Thyroid Carcinoma (THCA)',\n",
    "        'Stomach Adenocarcinoma (STAD)',\n",
    "        'Bladder Urothelial Carcinoma (BLCA)',\n",
    "        'Ovarian Serous Cystadenocarcinoma (OV)',\n",
    "        'Skin Cutaneous Melanoma (SKCM)',\n",
    "        'Liver Hepatocellular Carcinoma (LIHC)',\n",
    "        'Pancreatic Adenocarcinoma (PAAD)',\n",
    "        'Esophageal Carcinoma (ESCA)'\n",
    "    ]\n",
    "\n",
    "    # Selecting the top N cancer types for our dataset\n",
    "    selected_cancers = common_cancers[:__builtins__.min(n_cancer_types, len(common_cancers))]\n",
    "\n",
    "    # Creating a distribution that mimics real TCGA data\n",
    "    # More common cancers get more samples\n",
    "    cancer_distribution = {\n",
    "        selected_cancers[0]: 0.25,  # Most common (e.g., Breast)\n",
    "        selected_cancers[1]: 0.20,  # Second most common\n",
    "        selected_cancers[2]: 0.18,  # Third\n",
    "        selected_cancers[3]: 0.17,  # Fourth\n",
    "        selected_cancers[4]: 0.20   # Fifth (slightly more for balance)\n",
    "    }\n",
    "\n",
    "    # Assigning cancer types based on the distribution\n",
    "    clinical_data = []\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "\n",
    "    for i, sample_id in enumerate(sample_ids):\n",
    "        # Using the patient ID to deterministically assign cancer type\n",
    "        # This ensures the same patient always gets the same cancer type\n",
    "        patient_hash = __builtins__.hash(str(sample_id)) % 100 # Ensuring sample_id is string and use builtins.hash\n",
    "\n",
    "        if patient_hash < int(cancer_distribution[selected_cancers[0]] * 100):\n",
    "            cancer_type = selected_cancers[0]\n",
    "        elif patient_hash < int((cancer_distribution[selected_cancers[0]] + cancer_distribution[selected_cancers[1]]) * 100):\n",
    "            cancer_type = selected_cancers[1]\n",
    "        elif patient_hash < int((cancer_distribution[selected_cancers[0]] + cancer_distribution[selected_cancers[1]] + cancer_distribution[selected_cancers[2]]) * 100):\n",
    "            cancer_type = selected_cancers[2]\n",
    "        elif patient_hash < int((cancer_distribution[selected_cancers[0]] + cancer_distribution[selected_cancers[1]] + cancer_distribution[selected_cancers[2]] + cancer_distribution[selected_cancers[3]]) * 100):\n",
    "            cancer_type = selected_cancers[3]\n",
    "        else:\n",
    "            cancer_type = selected_cancers[4]\n",
    "\n",
    "        clinical_data.append({\n",
    "            'sample_id': sample_id,\n",
    "            'cancer_type': cancer_type,\n",
    "            'patient_id': sample_id  # Storing the original ID\n",
    "        })\n",
    "\n",
    "    clinical_df = pd.DataFrame(clinical_data)\n",
    "\n",
    "    print(\"Realistic cancer type labels created:\")\n",
    "    print(f\"Total samples: {len(clinical_df)}\")\n",
    "    print(f\"Cancer types: {n_cancer_types}\")\n",
    "    print(\"\\nCancer type distribution:\")\n",
    "    for cancer_type, count in clinical_df['cancer_type'].value_counts().items():\n",
    "        percentage = (count / len(clinical_df)) * 100\n",
    "        print(f\"  {cancer_type}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "    return clinical_df\n",
    "\n",
    "# Creating realistic clinical data\n",
    "clinical_data_realistic = create_realistic_cancer_labels(sample_ids, n_cancer_types=5)\n",
    "\n",
    "print(\"\\nClinical data preview:\")\n",
    "print(clinical_data_realistic.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19606,
     "status": "ok",
     "timestamp": 1764265188359,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "5aM9dXnRfZ0q",
    "outputId": "e37f417b-e32a-4998-bcaf-509d907d8974"
   },
   "outputs": [],
   "source": [
    "# Saving 'clinical_data_realistic' and 'rna_data_correct' directly before loading into Spark\n",
    "# This ensures the files exist if a previous save operation failed or files were cleaned up.\n",
    "print(\"üíæ Resaving raw data for Spark ingestion to ensure files exist...\")\n",
    "rna_data_correct.reset_index().rename(columns={'index': 'sample_id'}).to_csv(\n",
    "    \"/content/tcga_data/rna_raw_for_spark.csv\", index=False\n",
    ")\n",
    "clinical_data_realistic.to_csv(\n",
    "    \"/content/tcga_data/clinical_raw_for_spark.csv\", index=False\n",
    ")\n",
    "print(\"‚úÖ Raw data resaved!\")\n",
    "\n",
    "print(\"üì• Loading data into Spark DataFrames...\")\n",
    "rna_df_raw = spark.read.csv(\"/content/tcga_data/rna_raw_for_spark.csv\", header=True, inferSchema=True)\n",
    "clinical_df_raw = spark.read.csv(\"/content/tcga_data/clinical_raw_for_spark.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"üîó Merging DataFrames...\")\n",
    "# Ensure `spark` is available and `col` for join condition if needed, but `on` with string is fine.\n",
    "full_df = rna_df_raw.join(clinical_df_raw, on=\"sample_id\", how=\"inner\")\n",
    "\n",
    "# Handling Missing Values: Dropping rows that have any null values\n",
    "print(f\"Original sample count: {full_df.count()}\")\n",
    "clean_df = full_df.na.drop()\n",
    "print(f\"Clean sample count: {clean_df.count()}\")\n",
    "\n",
    "# Assign the cleaned and merged DataFrame to spark_df for subsequent cells\n",
    "spark_df = clean_df\n",
    "\n",
    "print(\"‚úÖ Data loaded and merged into Spark successfully!\")\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1764265188400,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "CCnZmyKEf5xS",
    "outputId": "5ec5ac4d-9db5-4901-9bfb-49098df4005d"
   },
   "outputs": [],
   "source": [
    " # Identifying columns\n",
    "# Separating feature columns (genes) from metadata (IDs, labels)\n",
    "ignore_cols = ['sample_id', 'cancer_type', 'patient_id']\n",
    "feature_cols = [c for c in spark_df.columns if c not in ignore_cols]\n",
    "\n",
    "# Defining the Stages of the Spark Pipeline\n",
    "\n",
    "# Stage A: Converting text labels (e.g., \"BRCA\") to numbers\n",
    "indexer = StringIndexer(inputCol=\"cancer_type\", outputCol=\"label\")\n",
    "\n",
    "# Stage B: Combining all gene columns into a single vector [cite: 18, 19]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"raw_features\")\n",
    "\n",
    "# Stage C: Standardizing features (Required for PCA) [cite: 21]\n",
    "from pyspark.ml.feature import StandardScaler # Re-importing to ensure correct class\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# Stage D: PCA for Feature Reduction [cite: 23, 24]\n",
    "# Reducing to top 100 components as you did in your Pandas version\n",
    "from pyspark.ml.feature import PCA # Re-importing to ensure correct class\n",
    "pca = PCA()\n",
    "pca.setK(100)\n",
    "pca.setInputCol(\"scaled_features\")\n",
    "pca.setOutputCol(\"pca_features\")\n",
    "\n",
    "# Stage E: Random Forest Classifier [cite: 29]\n",
    "from pyspark.ml.classification import RandomForestClassifier # Re-importing to ensure correct class\n",
    "rf_spark = RandomForestClassifier()\n",
    "rf_spark.setFeaturesCol(\"pca_features\")\n",
    "rf_spark.setLabelCol(\"label\")\n",
    "rf_spark.setNumTrees(100)\n",
    "rf_spark.setSeed(42)\n",
    "\n",
    "# 3. Creating the Pipeline\n",
    "pipeline = Pipeline(stages=[indexer, assembler, scaler, pca, rf_spark])\n",
    "\n",
    "print(\"‚úÖ Spark ML Pipeline constructed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131988,
     "status": "ok",
     "timestamp": 1764265320384,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "vLt72KqPgVK9",
    "outputId": "e0f34148-708a-4336-d0ce-52ec7c0f01b8"
   },
   "outputs": [],
   "source": [
    "# Splitting Data (70% Training, 30% Test) [cite: 26]\n",
    "train_data, test_data = spark_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(\"üöÄ Training Spark Model (this may take a moment)...\")\n",
    "\n",
    "# Training the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Making Predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluating [cite: 36, 37]\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"\\nüéâ Spark Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Showing confusion matrix equivalent\n",
    "predictions.groupBy(\"label\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2711,
     "status": "ok",
     "timestamp": 1764265323168,
     "user": {
      "displayName": "Ayodeji. Akomolafe.",
      "userId": "01678634609297189221"
     },
     "user_tz": -60
    },
    "id": "FRRJXvMpaEKK",
    "outputId": "d9d02e5e-c615-4470-c4e8-434637674628"
   },
   "outputs": [],
   "source": [
    "# Saving 'clinical_data_realistic' and 'rna_data_correct'\n",
    "# Saving them to disk so Spark can load them strictly as \"Raw Data\"\n",
    "print(\"üíæ Saving raw data for Spark ingestion...\")\n",
    "\n",
    "# Saving the transposed (but still dirty) RNA data\n",
    "# Reseting index to make sure the Sample IDs are a real column\n",
    "rna_data_correct.reset_index().rename(columns={'index': 'sample_id'}).to_csv(\n",
    "    \"/content/tcga_data/rna_raw_for_spark.csv\", index=False\n",
    ")\n",
    "\n",
    "# Saving the clinical labels\n",
    "clinical_data_realistic.to_csv(\n",
    "    \"/content/tcga_data/clinical_raw_for_spark.csv\", index=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Handoff complete! Ready for Spark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SPARK IMPLEMENTATION ===\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, PCA, StandardScaler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initializing Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TCGA_Genomic_Classifier\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"üöÄ Spark Session Initialized\")\n",
    "\n",
    "# Loading the CSVs we just saved\n",
    "print(\"üì• Loading data into Spark DataFrames...\")\n",
    "rna_df = spark.read.csv(\"/content/tcga_data/rna_raw_for_spark.csv\", header=True, inferSchema=True)\n",
    "clinical_df = spark.read.csv(\"/content/tcga_data/clinical_raw_for_spark.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# DATA CLEANING & MERGING\n",
    "# Merging the features (RNA) with labels (Clinical)\n",
    "print(\"üîó Merging DataFrames...\")\n",
    "full_df = rna_df.join(clinical_df, on=\"sample_id\", how=\"inner\")\n",
    "\n",
    "# Handling Missing Values\n",
    "# Dropping rows that have any null values (simple and effective for this dataset)\n",
    "print(f\"Original count: {full_df.count()}\")\n",
    "clean_df = full_df.na.drop()\n",
    "print(f\"Clean count: {clean_df.count()}\")\n",
    "\n",
    "# PREPARING THE PIPELINE\n",
    "print(\"‚öôÔ∏è Building Spark ML Pipeline...\")\n",
    "\n",
    "# Identifying gene columns (excluding metadata)\n",
    "exclude_cols = ['sample_id', 'cancer_type', 'patient_id', 'sample_id', '_c0'] # _c0 handles potential index artifacts\n",
    "gene_cols = [c for c in clean_df.columns if c not in exclude_cols]\n",
    "\n",
    "# Encoding Labels (String -> Index)\n",
    "indexer = StringIndexer(inputCol=\"cancer_type\", outputCol=\"label\")\n",
    "\n",
    "# Assembling Vector (Combine all gene columns into one vector)\n",
    "assembler = VectorAssembler(inputCols=gene_cols, outputCol=\"raw_features\")\n",
    "\n",
    "# Scaling Features (Required for PCA)\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# PCA\n",
    "# Reducing ~1000 features to 50 principal components\n",
    "pca = PCA(k=50, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Classifier (Random Forest)\n",
    "rf = RandomForestClassifier(featuresCol=\"pca_features\", labelCol=\"label\",\n",
    "                            numTrees=100, seed=42)\n",
    "\n",
    "# Creating the full pipeline\n",
    "pipeline = Pipeline(stages=[indexer, assembler, scaler, pca, rf])\n",
    "\n",
    "# === DUAL MODEL TRAINING & COMPARISON ===\n",
    "from pyspark.ml.classification import RandomForestClassifier, LinearSVC, OneVsRest\n",
    "\n",
    "print(\"üèãÔ∏è Training & Comparing Models...\")\n",
    "\n",
    "# SPLITTING DATA (Guideline 4.1)\n",
    "train_data, test_data = clean_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# DEFINING MODEL ESTIMATORS\n",
    "# Model A: Random Forest (Natively supports multiclass)\n",
    "rf = RandomForestClassifier(featuresCol=\"pca_features\", labelCol=\"label\",\n",
    "                            numTrees=100, seed=42)\n",
    "\n",
    "# Model B: Linear SVM (Requires OneVsRest for multiclass support)\n",
    "# Wrapping the LinearSVC because standard SVM in Spark is binary-only\n",
    "l_svc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "ovr_svm = OneVsRest(classifier=l_svc, featuresCol=\"pca_features\", labelCol=\"label\")\n",
    "\n",
    "# BUILDING PIPELINES\n",
    "# Reusing the same preprocessing stages (0-3) for both models to be fair\n",
    "preprocessing_stages = [indexer, assembler, scaler, pca]\n",
    "\n",
    "pipeline_rf = Pipeline(stages=preprocessing_stages + [rf])\n",
    "pipeline_svm = Pipeline(stages=preprocessing_stages + [ovr_svm])\n",
    "\n",
    "# ==============================================================================\n",
    "# TUNING\n",
    "# ==============================================================================\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "print(\"‚ö° Starting ULTRA-FAST Hyperparameter Tuning...\")\n",
    "\n",
    "# PRE-PROCESSING DATA ONCE (The heavy lifting)\n",
    "# We fit the preprocessing part (Scaler + PCA) just one time.\n",
    "print(\"   -> Running PCA Preprocessing (One-time cost)...\")\n",
    "prep_pipeline = Pipeline(stages=preprocessing_stages)\n",
    "prep_model = prep_pipeline.fit(train_data)\n",
    "\n",
    "# Transforming the data into \"pca_features\"\n",
    "# We select only the columns we need for the classifier to save memory\n",
    "train_pca = prep_model.transform(train_data).select(\"sample_id\", \"pca_features\", \"label\")\n",
    "test_pca = prep_model.transform(test_data).select(\"sample_id\", \"pca_features\", \"label\")\n",
    "\n",
    "# CACHING THE TRANSFORMED DATA\n",
    "# Now the data is small (50 features) and ready for instant training\n",
    "train_pca.cache()\n",
    "print(f\"   -> Pre-processed data cached. Count: {train_pca.count()}\")\n",
    "\n",
    "# DEFINING ONLY THE CLASSIFIER (No Pipeline overhead)\n",
    "rf = RandomForestClassifier(featuresCol=\"pca_features\", labelCol=\"label\", seed=42)\n",
    "\n",
    "# LIGHTWEIGHT GRID (1 vs 5 trees)\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [1, 5]) \\\n",
    "    .build()\n",
    "\n",
    "# FAST VALIDATOR\n",
    "# Tuning ONLY the Random Forest, not the whole pipeline\n",
    "tvs = TrainValidationSplit(estimator=rf,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"),\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# RUNNING TUNING \n",
    "print(\"   -> Tuning classifier...\")\n",
    "tvsModel = tvs.fit(train_pca)\n",
    "\n",
    "# EXTRACTING BEST MODEL\n",
    "best_rf = tvsModel.bestModel\n",
    "print(f\"‚úÖ Best Params Found: NumTrees={best_rf.getNumTrees}\")\n",
    "\n",
    "# PREDICTIONS & EVALUATION\n",
    "# Note: We use the pre-processed 'test_pca' here\n",
    "print(\"   -> Generating final predictions...\")\n",
    "preds_rf = best_rf.transform(test_pca)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acc_rf = evaluator.evaluate(preds_rf)\n",
    "print(f\"   -> Random Forest Accuracy: {acc_rf:.2%}\")\n",
    "\n",
    "# UPDATING GLOBAL MODEL VARIABLES\n",
    "# We need to reconstruct the full pipeline so the Ensemble code works later\n",
    "# We combine the pre-trained PCA model with the tuned RF model\n",
    "model_rf = PipelineModel(stages=prep_model.stages + [best_rf])\n",
    "\n",
    "# SVM SECTION (Standard)\n",
    "print(\"‚öîÔ∏è Training Support Vector Machine (OneVsRest)...\")\n",
    "# Using the faster 'train_pca' here!\n",
    "l_svc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "ovr_svm = OneVsRest(classifier=l_svc, featuresCol=\"pca_features\", labelCol=\"label\")\n",
    "model_svm_only = ovr_svm.fit(train_pca)\n",
    "preds_svm = model_svm_only.transform(test_pca)\n",
    "\n",
    "# Reconstructing full SVM pipeline for consistency\n",
    "model_svm = PipelineModel(stages=prep_model.stages + [model_svm_only])\n",
    "\n",
    "acc_svm = evaluator.evaluate(preds_svm)\n",
    "print(f\"   -> SVM Accuracy: {acc_svm:.2%}\")\n",
    "\n",
    "# FINAL COMPARISON\n",
    "print(\"\\nüèÜ MODEL SHOWDOWN RESULTS\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Random Forest: {acc_rf:.4f}\")\n",
    "print(f\"SVM (OvR):     {acc_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRuswa0beHc9"
   },
   "outputs": [],
   "source": [
    "# === REPORTING, VISUALIZATION & BIOMARKERS ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "print(\"üî¨ GENERATING PROJECT REPORT VISUALIZATIONS...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. BIOMARKER IDENTIFICATION (Using Random Forest)\n",
    "# ---------------------------------------------------------\n",
    "# We use RF for biomarkers because it gives a global importance score.\n",
    "# SVM (OneVsRest) gives complex class-specific coefficients.\n",
    "print(\"\\nüß¨ Identifying Top Biomarkers (derived from Random Forest)...\")\n",
    "\n",
    "# Accessing the stages from the RF pipeline (Index 3=PCA, Index 4=RF)\n",
    "# Note: We explicitly use 'model_rf' here, even if SVM won the accuracy battle,\n",
    "# because RF is better for explaining feature importance.\n",
    "pca_stage = model_rf.stages[3]\n",
    "rf_stage = model_rf.stages[4]\n",
    "\n",
    "# A. Getting PCA Component Importance (from RF) and Gene Weights (from PCA)\n",
    "pc_importances = rf_stage.featureImportances.toArray()\n",
    "pc_weights = pca_stage.pc.toArray()\n",
    "\n",
    "# B. Calculating Gene Importance\n",
    "# Dot product of (Gene weights in PCs) * (Importance of those PCs)\n",
    "gene_importance_scores = np.dot(np.abs(pc_weights), pc_importances)\n",
    "\n",
    "# C. Mapping to Gene Names\n",
    "# 'gene_cols' must exist from Phase 2. If not, we use generic names.\n",
    "if 'gene_cols' not in globals():\n",
    "    print(\"Warning: 'gene_cols' variable not found. Using generic IDs.\")\n",
    "    gene_cols = [f\"Gene_{i}\" for i in range(len(gene_importance_scores))]\n",
    "\n",
    "biomarker_df = pd.DataFrame({\n",
    "    'Gene': gene_cols,\n",
    "    'Importance_Score': gene_importance_scores\n",
    "})\n",
    "\n",
    "# D. Showing Top 20\n",
    "top_biomarkers = biomarker_df.sort_values(by='Importance_Score', ascending=False).head(20)\n",
    "print(top_biomarkers)\n",
    "\n",
    "# Plotting Biomarkers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance_Score', y='Gene', data=top_biomarkers, palette='viridis')\n",
    "plt.title('Top 20 Potential Cancer Biomarkers (RF Derived)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. MODEL COMPARISON VISUALIZATION (RF vs SVM)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nüìä Generating Confusion Matrices for Comparison...\")\n",
    "\n",
    "def get_confusion_matrix(predictions, model_name):\n",
    "    # Grouping by Label and Prediction in Spark\n",
    "    cm_spark = predictions.groupBy(\"label\", \"prediction\").count().toPandas()\n",
    "    # Pivoting to Matrix format\n",
    "    cm_matrix = cm_spark.pivot(index='label', columns='prediction', values='count').fillna(0)\n",
    "    # Sorting index to ensure 0,1,2,3,4 order\n",
    "    cm_matrix = cm_matrix.sort_index(axis=0).sort_index(axis=1)\n",
    "    return cm_matrix\n",
    "\n",
    "# Getting matrices for both\n",
    "cm_rf = get_confusion_matrix(preds_rf, \"Random Forest\")\n",
    "cm_svm = get_confusion_matrix(preds_svm, \"SVM\")\n",
    "\n",
    "# Getting class names for labeling\n",
    "class_names = indexer.fit(clean_df).labels\n",
    "\n",
    "# Plotting Side-by-Side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest Plot\n",
    "sns.heatmap(cm_rf, annot=True, fmt='g', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[0].set_title(f'Random Forest Confusion Matrix\\nAccuracy: {acc_rf:.2%}')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# SVM Plot\n",
    "sns.heatmap(cm_svm, annot=True, fmt='g', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[1].set_title(f'SVM (OneVsRest) Confusion Matrix\\nAccuracy: {acc_svm:.2%}')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. PCA VARIANCE EXPLAINED\n",
    "# ---------------------------------------------------------\n",
    "# Validates why we chose k=50 components\n",
    "explained_var = pca_stage.explainedVariance.toArray()\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(explained_var) + 1), np.cumsum(explained_var), marker='o')\n",
    "plt.title('Cumulative Variance Explained by PCA Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ PROJECT COMPLETE: Results Visualized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENSEMBLE LEARNING (VOTING CLASSIFIER)\n",
    "# ==============================================================================\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf, col, array\n",
    "from collections import Counter\n",
    "\n",
    "print(\"ü§ù Building Ensemble Model (Voting Classifier)...\")\n",
    "\n",
    "# TRAINING A 3RD MODEL (LOGISTIC REGRESSION) FOR TIE-BREAKING\n",
    "# ------------------------------------------------------------------\n",
    "# We need an odd number of models to avoid ties (e.g., SVM says \"A\", RF says \"B\").\n",
    "print(\"   -> Training Tie-Breaker Model (Logistic Regression)...\")\n",
    "lr = LogisticRegression(featuresCol=\"pca_features\", labelCol=\"label\", maxIter=10)\n",
    "pipeline_lr = Pipeline(stages=preprocessing_stages + [lr])\n",
    "model_lr = pipeline_lr.fit(train_data)\n",
    "\n",
    "# GENERATING PREDICTIONS FROM ALL 3 MODELS\n",
    "# ------------------------------------------------------------------\n",
    "print(\"   -> Gathering votes from RF, SVM, and LR...\")\n",
    "\n",
    "# Getting predictions and rename the prediction columns to avoid confusion\n",
    "preds_rf_clean = model_rf.transform(test_data).withColumnRenamed(\"prediction\", \"pred_rf\")\n",
    "preds_svm_clean = model_svm.transform(test_data).withColumnRenamed(\"prediction\", \"pred_svm\")\n",
    "preds_lr_clean = model_lr.transform(test_data).withColumnRenamed(\"prediction\", \"pred_lr\")\n",
    "\n",
    "# Joining them into a single DataFrame for voting\n",
    "# We join on 'sample_id' to ensure we are voting on the correct patient\n",
    "votes_df = preds_rf_clean.select(\"sample_id\", \"label\", \"pred_rf\") \\\n",
    "    .join(preds_svm_clean.select(\"sample_id\", \"pred_svm\"), on=\"sample_id\") \\\n",
    "    .join(preds_lr_clean.select(\"sample_id\", \"pred_lr\"), on=\"sample_id\")\n",
    "\n",
    "# DEFINING THE VOTING LOGIC (UDF)\n",
    "# ------------------------------------------------------------------\n",
    "# This function takes a list of votes and returns the most common one (Mode)\n",
    "def get_majority_vote(votes):\n",
    "    # votes is a list like [1.0, 1.0, 2.0] -> Returns 1.0\n",
    "    # If there is a 3-way tie, it defaults to the first model's choice (RF)\n",
    "    counts = Counter(votes)\n",
    "    return float(counts.most_common(1)[0][0])\n",
    "\n",
    "# Registering the UDF with Spark\n",
    "vote_udf = udf(get_majority_vote, DoubleType())\n",
    "\n",
    "# APPLYING VOTING TO DATA\n",
    "# ------------------------------------------------------------------\n",
    "print(\"   -> Calculating majority votes...\")\n",
    "\n",
    "# Creating a new column 'ensemble_prediction' based on the 3 model columns\n",
    "ensemble_df = votes_df.withColumn(\"ensemble_prediction\",\n",
    "                                  vote_udf(array(\"pred_rf\", \"pred_svm\", \"pred_lr\")))\n",
    "\n",
    "# EVALUATING ENSEMBLE PERFORMANCE\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\nüèÜ ENSEMBLE RESULTS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"ensemble_prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "ensemble_acc = evaluator.evaluate(ensemble_df)\n",
    "\n",
    "# Comparing with individual models\n",
    "print(f\"Random Forest Accuracy:     {acc_rf:.2%}\")\n",
    "print(f\"SVM Accuracy:               {acc_svm:.2%}\")\n",
    "print(f\"Ensemble (Voting) Accuracy: {ensemble_acc:.2%}\")\n",
    "\n",
    "if ensemble_acc > __builtins__.max(acc_rf, acc_svm):\n",
    "    print(\"‚úÖ SUCCESS: Ensemble outperformed individual models!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Note: Ensemble performed similarly to the best single model.\")\n",
    "\n",
    "# Showing a sample of the voting process\n",
    "print(\"\\nSample Voting Breakdown:\")\n",
    "ensemble_df.select(\"label\", \"pred_rf\", \"pred_svm\", \"pred_lr\", \"ensemble_prediction\").show(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
